{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ“ All packages available!\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "import pandas, numpy, matplotlib, seaborn, scipy\n",
        "print(\"âœ“ All packages available!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Datathon EDA Template\n",
        "\n",
        "**Team Name:** [Your Team Name]  \n",
        "**Date:** [Date]  \n",
        "**Competition:** [Competition Name]  \n",
        "**Dataset:** [Dataset Name/Description]  \n",
        "\n",
        "---\n",
        "\n",
        "## Team Workflow Strategy\n",
        "\n",
        "**Phase 1 (0-15 min): Together**\n",
        "- Run Sections 0 & 1 as a team\n",
        "- Discuss problem context and target variable\n",
        "- Align on objectives\n",
        "\n",
        "**Phase 2 (15-45 min): Parallel Work**\n",
        "- **Member 1:** Sections 2 & 3 (Data quality and univariate analysis)\n",
        "- **Member 2:** Section 4 (Bivariate relationships)\n",
        "- **Member 3:** Section 5 (Multivariate patterns and modeling prep)\n",
        "\n",
        "**Phase 3 (45-60 min): Together**\n",
        "- Share findings (5 min each)\n",
        "- Section 6: Brainstorm features together\n",
        "- Section 7: Plan modeling strategy\n",
        "- Assign next tasks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 0: Setup & Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', 50)\n",
        "\n",
        "# Visualization settings\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Initialize findings dictionary for systematic documentation\n",
        "findings = {\n",
        "    'data_quality_issues': [],\n",
        "    'key_insights': [],\n",
        "    'feature_ideas': [],\n",
        "    'questions_for_team': [],\n",
        "    'next_steps': []\n",
        "}\n",
        "\n",
        "print(\"âœ“ Section 0: Setup & Imports completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Load Data & Initial Inspection\n",
        "\n",
        "**Team Activity:** Run together and discuss problem context\n",
        "\n",
        "**âš ï¸ IMPORTANT:** Before running this section, uncomment and modify the data loading line below to load your dataset!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "# Uncomment and modify the appropriate line for your data source\n",
        "\n",
        "# Option 1: CSV file\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Option 2: Excel file\n",
        "# df = pd.read_excel('your_dataset.xlsx')\n",
        "\n",
        "# Option 3: JSON file\n",
        "# df = pd.read_json('your_dataset.json')\n",
        "\n",
        "# Option 4: From URL\n",
        "# df = pd.read_csv('https://example.com/dataset.csv')\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"DATASET OVERVIEW\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Basic information\n",
        "print(f\"\\nDataset Shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
        "print(f\"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"FIRST 5 ROWS\")\n",
        "print(\"=\" * 80)\n",
        "display(df.head())\n",
        "\n",
        "# Display last few rows\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"LAST 5 ROWS\")\n",
        "print(\"=\" * 80)\n",
        "display(df.tail())\n",
        "\n",
        "# Random sample\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"RANDOM SAMPLE (5 ROWS)\")\n",
        "print(\"=\" * 80)\n",
        "display(df.sample(5, random_state=42))\n",
        "\n",
        "# Column information\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"COLUMN INFORMATION\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nColumn Names ({len(df.columns)}):\")\n",
        "for i, col in enumerate(df.columns, 1):\n",
        "    print(f\"  {i:2d}. {col}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DATA TYPES\")\n",
        "print(\"=\" * 80)\n",
        "display(df.dtypes.to_frame('Data Type'))\n",
        "\n",
        "# Info summary\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DETAILED INFO\")\n",
        "print(\"=\" * 80)\n",
        "df.info()\n",
        "\n",
        "# Initial observations\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"INITIAL OBSERVATIONS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nâ€¢ Total features: {df.shape[1]}\")\n",
        "print(f\"â€¢ Total samples: {df.shape[0]:,}\")\n",
        "print(f\"â€¢ Numerical columns: {len(df.select_dtypes(include=[np.number]).columns)}\")\n",
        "print(f\"â€¢ Categorical columns: {len(df.select_dtypes(include=['object', 'category']).columns)}\")\n",
        "print(f\"â€¢ Datetime columns: {len(df.select_dtypes(include=['datetime64']).columns)}\")\n",
        "\n",
        "# Document initial observations\n",
        "findings['key_insights'].append(f\"Dataset contains {df.shape[0]:,} rows and {df.shape[1]} columns\")\n",
        "findings['key_insights'].append(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "print(\"\\nâœ“ Section 1: Load Data & Initial Inspection completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Data Quality Assessment\n",
        "\n",
        "**Assigned to: Member 1**  \n",
        "**Time: 15-30 minutes**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"DATA QUALITY ASSESSMENT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 2.1 Missing Values Analysis\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"2.1 MISSING VALUES ANALYSIS\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "missing_data = df.isnull().sum()\n",
        "missing_percent = (missing_data / len(df)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Column': missing_data.index,\n",
        "    'Missing Count': missing_data.values,\n",
        "    'Missing Percentage': missing_percent.values\n",
        "})\n",
        "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
        "\n",
        "if len(missing_df) > 0:\n",
        "    display(missing_df)\n",
        "    \n",
        "    # Visualize missing values\n",
        "    plt.figure(figsize=(12, max(6, len(missing_df) * 0.5)))\n",
        "    sns.barplot(data=missing_df, y='Column', x='Missing Percentage', palette='Reds_r')\n",
        "    plt.title('Missing Values by Column', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Missing Percentage (%)', fontsize=12)\n",
        "    plt.ylabel('Column', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Document findings\n",
        "    for _, row in missing_df.iterrows():\n",
        "        findings['data_quality_issues'].append(\n",
        "            f\"{row['Column']}: {row['Missing Count']:,} missing values ({row['Missing Percentage']:.2f}%)\"\n",
        "        )\n",
        "else:\n",
        "    print(\"âœ“ No missing values found in the dataset\")\n",
        "    findings['key_insights'].append(\"No missing values detected in the dataset\")\n",
        "\n",
        "# 2.2 Duplicate Rows\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"2.2 DUPLICATE ROWS\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(f\"Total duplicate rows: {duplicate_count:,} ({duplicate_count/len(df)*100:.2f}%)\")\n",
        "\n",
        "if duplicate_count > 0:\n",
        "    print(\"\\nSample duplicate rows:\")\n",
        "    display(df[df.duplicated(keep=False)].head(10))\n",
        "    findings['data_quality_issues'].append(f\"{duplicate_count:,} duplicate rows found ({duplicate_count/len(df)*100:.2f}%)\")\n",
        "else:\n",
        "    print(\"âœ“ No duplicate rows found\")\n",
        "    findings['key_insights'].append(\"No duplicate rows detected\")\n",
        "\n",
        "# 2.3 Data Type Verification\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"2.3 DATA TYPE VERIFICATION\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Check for columns that might be incorrectly typed\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()\n",
        "\n",
        "print(f\"Numerical columns: {len(numeric_cols)}\")\n",
        "print(f\"Categorical columns: {len(categorical_cols)}\")\n",
        "print(f\"Datetime columns: {len(datetime_cols)}\")\n",
        "\n",
        "# Check for mixed types in object columns\n",
        "print(\"\\nChecking for mixed types in object columns...\")\n",
        "mixed_type_cols = []\n",
        "for col in categorical_cols:\n",
        "    # Try to convert to numeric and see if there are any numeric values\n",
        "    numeric_vals = pd.to_numeric(df[col], errors='coerce')\n",
        "    if numeric_vals.notna().sum() > 0 and numeric_vals.notna().sum() < len(df):\n",
        "        mixed_type_cols.append(col)\n",
        "        findings['data_quality_issues'].append(f\"{col}: Mixed data types detected (numeric and non-numeric)\")\n",
        "\n",
        "if mixed_type_cols:\n",
        "    print(f\"âš  Found {len(mixed_type_cols)} columns with mixed types: {mixed_type_cols}\")\n",
        "else:\n",
        "    print(\"âœ“ No mixed type columns detected\")\n",
        "\n",
        "# 2.4 Constant Columns\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"2.4 CONSTANT COLUMNS\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "constant_cols = []\n",
        "for col in df.columns:\n",
        "    if df[col].nunique() <= 1:\n",
        "        constant_cols.append(col)\n",
        "        findings['data_quality_issues'].append(f\"{col}: Constant column (only {df[col].nunique()} unique value)\")\n",
        "\n",
        "if constant_cols:\n",
        "    print(f\"âš  Found {len(constant_cols)} constant columns: {constant_cols}\")\n",
        "else:\n",
        "    print(\"âœ“ No constant columns found\")\n",
        "\n",
        "# 2.5 Data Quality Summary\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DATA QUALITY SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nTotal columns: {df.shape[1]}\")\n",
        "print(f\"Columns with missing values: {len(missing_df) if len(missing_df) > 0 else 0}\")\n",
        "print(f\"Duplicate rows: {duplicate_count:,}\")\n",
        "print(f\"Mixed type columns: {len(mixed_type_cols)}\")\n",
        "print(f\"Constant columns: {len(constant_cols)}\")\n",
        "print(f\"Total data quality issues: {len(findings['data_quality_issues'])}\")\n",
        "\n",
        "print(\"\\nâœ“ Section 2: Data Quality Assessment completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Univariate Analysis\n",
        "\n",
        "**Assigned to: Member 1**  \n",
        "**Time: 15-30 minutes**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"UNIVARIATE ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 3.1 Separate Numerical and Categorical Columns\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "print(f\"\\nNumerical columns ({len(numeric_cols)}): {numeric_cols}\")\n",
        "print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
        "\n",
        "# 3.2 Descriptive Statistics for Numerical Variables\n",
        "if len(numeric_cols) > 0:\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "    print(\"3.2 DESCRIPTIVE STATISTICS (NUMERICAL)\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    desc_stats = df[numeric_cols].describe().T\n",
        "    desc_stats['skewness'] = df[numeric_cols].skew()\n",
        "    desc_stats['kurtosis'] = df[numeric_cols].kurtosis()\n",
        "    desc_stats['missing_count'] = df[numeric_cols].isnull().sum()\n",
        "    desc_stats['missing_pct'] = (desc_stats['missing_count'] / len(df)) * 100\n",
        "    \n",
        "    display(desc_stats)\n",
        "    \n",
        "    # Document insights about distributions\n",
        "    for col in numeric_cols:\n",
        "        skew_val = desc_stats.loc[col, 'skewness']\n",
        "        if abs(skew_val) > 1:\n",
        "            findings['key_insights'].append(\n",
        "                f\"{col}: Highly {'right' if skew_val > 0 else 'left'}-skewed distribution (skewness={skew_val:.2f})\"\n",
        "            )\n",
        "\n",
        "# 3.3 Outlier Detection (IQR Method)\n",
        "if len(numeric_cols) > 0:\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "    print(\"3.3 OUTLIER DETECTION (IQR METHOD)\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    outlier_summary = []\n",
        "    \n",
        "    for col in numeric_cols:\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        \n",
        "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n",
        "        outlier_count = len(outliers)\n",
        "        outlier_pct = (outlier_count / len(df)) * 100\n",
        "        \n",
        "        if outlier_count > 0:\n",
        "            outlier_summary.append({\n",
        "                'Column': col,\n",
        "                'Outlier Count': outlier_count,\n",
        "                'Outlier Percentage': outlier_pct,\n",
        "                'Lower Bound': lower_bound,\n",
        "                'Upper Bound': upper_bound\n",
        "            })\n",
        "            \n",
        "            if outlier_pct > 5:  # Flag if more than 5% outliers\n",
        "                findings['data_quality_issues'].append(\n",
        "                    f\"{col}: {outlier_count:,} outliers ({outlier_pct:.2f}%) detected using IQR method\"\n",
        "                )\n",
        "    \n",
        "    if outlier_summary:\n",
        "        outlier_df = pd.DataFrame(outlier_summary)\n",
        "        display(outlier_df)\n",
        "    else:\n",
        "        print(\"âœ“ No significant outliers detected using IQR method\")\n",
        "\n",
        "# 3.4 Categorical Variable Analysis\n",
        "if len(categorical_cols) > 0:\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "    print(\"3.4 CATEGORICAL VARIABLE CARDINALITY\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    cat_summary = []\n",
        "    for col in categorical_cols:\n",
        "        unique_count = df[col].nunique()\n",
        "        cat_summary.append({\n",
        "            'Column': col,\n",
        "            'Unique Values': unique_count,\n",
        "            'Most Frequent': df[col].mode()[0] if len(df[col].mode()) > 0 else 'N/A',\n",
        "            'Most Frequent Count': df[col].value_counts().iloc[0] if unique_count > 0 else 0,\n",
        "            'Most Frequent %': (df[col].value_counts().iloc[0] / len(df) * 100) if unique_count > 0 else 0\n",
        "        })\n",
        "        \n",
        "        # Document high cardinality\n",
        "        if unique_count > 50:\n",
        "            findings['key_insights'].append(\n",
        "                f\"{col}: High cardinality categorical variable ({unique_count} unique values)\"\n",
        "            )\n",
        "    \n",
        "    cat_df = pd.DataFrame(cat_summary)\n",
        "    display(cat_df)\n",
        "\n",
        "# 3.5 Distribution Visualizations - Numerical\n",
        "if len(numeric_cols) > 0:\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "    print(\"3.5 DISTRIBUTION VISUALIZATIONS (NUMERICAL)\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    # Calculate grid dimensions\n",
        "    n_cols = min(3, len(numeric_cols))\n",
        "    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
        "    \n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
        "    axes = axes.flatten() if len(numeric_cols) > 1 else [axes]\n",
        "    \n",
        "    for idx, col in enumerate(numeric_cols):\n",
        "        ax = axes[idx]\n",
        "        df[col].hist(bins=50, ax=ax, edgecolor='black', alpha=0.7)\n",
        "        ax.set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
        "        ax.set_xlabel(col, fontsize=10)\n",
        "        ax.set_ylabel('Frequency', fontsize=10)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Hide extra subplots\n",
        "    for idx in range(len(numeric_cols), len(axes)):\n",
        "        axes[idx].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 3.6 Value Counts - Categorical\n",
        "if len(categorical_cols) > 0:\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "    print(\"3.6 VALUE COUNTS (CATEGORICAL - TOP 10)\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    for col in categorical_cols[:5]:  # Limit to first 5 to avoid too much output\n",
        "        print(f\"\\n{col}:\")\n",
        "        value_counts = df[col].value_counts().head(10)\n",
        "        display(value_counts.to_frame('Count'))\n",
        "        \n",
        "        # Visualize top categories\n",
        "        if df[col].nunique() <= 20:  # Only plot if reasonable number of categories\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            value_counts.plot(kind='bar')\n",
        "            plt.title(f'Value Counts: {col}', fontsize=12, fontweight='bold')\n",
        "            plt.xlabel(col, fontsize=10)\n",
        "            plt.ylabel('Count', fontsize=10)\n",
        "            plt.xticks(rotation=45, ha='right')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "print(\"\\nâœ“ Section 3: Univariate Analysis completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Bivariate Analysis\n",
        "\n",
        "**Assigned to: Member 2**  \n",
        "**Time: 15-30 minutes**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"BIVARIATE ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 4.1 Correlation Matrix\n",
        "if len(numeric_cols) > 1:\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "    print(\"4.1 CORRELATION MATRIX\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    correlation_matrix = df[numeric_cols].corr()\n",
        "    \n",
        "    # Display correlation matrix\n",
        "    display(correlation_matrix)\n",
        "    \n",
        "    # Visualize correlation heatmap\n",
        "    plt.figure(figsize=(max(10, len(numeric_cols)), max(8, len(numeric_cols))))\n",
        "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))  # Mask upper triangle\n",
        "    sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', \n",
        "                center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "    plt.title('Correlation Matrix (Numerical Features)', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 4.2 High Correlation Detection\n",
        "if len(numeric_cols) > 1:\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "    print(\"4.2 HIGH CORRELATION PAIRS (|r| > 0.7)\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    high_corr_pairs = []\n",
        "    for i in range(len(correlation_matrix.columns)):\n",
        "        for j in range(i+1, len(correlation_matrix.columns)):\n",
        "            col1 = correlation_matrix.columns[i]\n",
        "            col2 = correlation_matrix.columns[j]\n",
        "            corr_val = correlation_matrix.iloc[i, j]\n",
        "            \n",
        "            if abs(corr_val) > 0.7:\n",
        "                high_corr_pairs.append({\n",
        "                    'Feature 1': col1,\n",
        "                    'Feature 2': col2,\n",
        "                    'Correlation': corr_val\n",
        "                })\n",
        "                findings['key_insights'].append(\n",
        "                    f\"High correlation between {col1} and {col2}: {corr_val:.3f}\"\n",
        "                )\n",
        "    \n",
        "    if high_corr_pairs:\n",
        "        high_corr_df = pd.DataFrame(high_corr_pairs)\n",
        "        display(high_corr_df.sort_values('Correlation', key=abs, ascending=False))\n",
        "    else:\n",
        "        print(\"âœ“ No highly correlated pairs found (|r| > 0.7)\")\n",
        "\n",
        "# 4.3 Target Variable Relationship Analysis\n",
        "# Uncomment and modify if you have a target variable\n",
        "\n",
        "# target_col = 'your_target_column_name'  # SET YOUR TARGET COLUMN HERE\n",
        "# \n",
        "# if target_col in df.columns:\n",
        "#     print(\"\\n\" + \"-\" * 80)\n",
        "#     print(f\"4.3 TARGET VARIABLE ANALYSIS: {target_col}\")\n",
        "#     print(\"-\" * 80)\n",
        "#     \n",
        "#     # If target is numerical\n",
        "#     if df[target_col].dtype in [np.number]:\n",
        "#         # Correlation with target\n",
        "#         target_corr = df[numeric_cols].corrwith(df[target_col]).sort_values(key=abs, ascending=False)\n",
        "#         print(\"\\nCorrelation with target:\")\n",
        "#         display(target_corr.to_frame('Correlation'))\n",
        "#         \n",
        "#         # Top correlated features\n",
        "#         top_features = target_corr.abs().nlargest(10).index.tolist()\n",
        "#         findings['key_insights'].append(f\"Top features correlated with {target_col}: {top_features[:5]}\")\n",
        "#         \n",
        "#         # Scatter plots for top features\n",
        "#         n_top = min(6, len(top_features))\n",
        "#         fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "#         axes = axes.flatten()\n",
        "#         \n",
        "#         for idx, feature in enumerate(top_features[:n_top]):\n",
        "#             ax = axes[idx]\n",
        "#             ax.scatter(df[feature], df[target_col], alpha=0.5, s=20)\n",
        "#             ax.set_xlabel(feature, fontsize=10)\n",
        "#             ax.set_ylabel(target_col, fontsize=10)\n",
        "#             ax.set_title(f'{feature} vs {target_col}\\n(r={target_corr[feature]:.3f})', fontsize=11)\n",
        "#             ax.grid(True, alpha=0.3)\n",
        "#         \n",
        "#         plt.tight_layout()\n",
        "#         plt.show()\n",
        "#     \n",
        "#     # If target is categorical\n",
        "#     elif df[target_col].dtype in ['object', 'category']:\n",
        "#         # Class distribution\n",
        "#         print(\"\\nTarget class distribution:\")\n",
        "#         target_dist = df[target_col].value_counts()\n",
        "#         display(target_dist.to_frame('Count'))\n",
        "#         \n",
        "#         # Visualize class distribution\n",
        "#         plt.figure(figsize=(10, 6))\n",
        "#         target_dist.plot(kind='bar')\n",
        "#         plt.title(f'Distribution of {target_col}', fontsize=12, fontweight='bold')\n",
        "#         plt.xlabel(target_col, fontsize=10)\n",
        "#         plt.ylabel('Count', fontsize=10)\n",
        "#         plt.xticks(rotation=45, ha='right')\n",
        "#         plt.tight_layout()\n",
        "#         plt.show()\n",
        "#         \n",
        "#         # Box plots for numerical features by target class\n",
        "#         if len(numeric_cols) > 0:\n",
        "#             n_features = min(6, len(numeric_cols))\n",
        "#             fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "#             axes = axes.flatten()\n",
        "#             \n",
        "#             for idx, feature in enumerate(numeric_cols[:n_features]):\n",
        "#                 ax = axes[idx]\n",
        "#                 df.boxplot(column=feature, by=target_col, ax=ax)\n",
        "#                 ax.set_title(f'{feature} by {target_col}', fontsize=11)\n",
        "#                 ax.set_xlabel(target_col, fontsize=10)\n",
        "#                 ax.set_ylabel(feature, fontsize=10)\n",
        "#                 plt.suptitle('')  # Remove default title\n",
        "#             \n",
        "#             plt.tight_layout()\n",
        "#             plt.show()\n",
        "# else:\n",
        "#     print(f\"\\nâš  Target column '{target_col}' not found in dataset\")\n",
        "\n",
        "print(\"\\nâœ“ Section 4: Bivariate Analysis completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Multivariate Analysis & Patterns\n",
        "\n",
        "**Assigned to: Member 3**  \n",
        "**Time: 15-30 minutes**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"MULTIVARIATE ANALYSIS & PATTERNS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 5.1 Pairplot for Pattern Detection\n",
        "if len(numeric_cols) > 1 and len(numeric_cols) <= 8:  # Limit to avoid too many plots\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "    print(\"5.1 PAIRPLOT (PATTERN DETECTION)\")\n",
        "    print(\"-\" * 80)\n",
        "    print(\"\\nGenerating pairplot (this may take a moment)...\")\n",
        "    \n",
        "    # Sample data if too large for pairplot\n",
        "    sample_size = min(1000, len(df))\n",
        "    df_sample = df[numeric_cols].sample(n=sample_size, random_state=42) if len(df) > 1000 else df[numeric_cols]\n",
        "    \n",
        "    # Create pairplot\n",
        "    sns.pairplot(df_sample, diag_kind='kde', plot_kws={'alpha': 0.6, 's': 20})\n",
        "    plt.suptitle('Pairplot of Numerical Features', y=1.02, fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    findings['key_insights'].append(\"Pairplot generated to identify multivariate patterns and clusters\")\n",
        "elif len(numeric_cols) > 8:\n",
        "    print(\"\\nâš  Too many numerical columns for pairplot. Consider selecting key features.\")\n",
        "    findings['questions_for_team'].append(\"Which numerical features should we focus on for multivariate analysis?\")\n",
        "\n",
        "# 5.2 Class Imbalance Detection (if target exists)\n",
        "# Uncomment if you have a target variable\n",
        "# if 'target_col' in locals() and target_col in df.columns:\n",
        "#     if df[target_col].dtype in ['object', 'category']:\n",
        "#         print(\"\\n\" + \"-\" * 80)\n",
        "#         print(\"5.2 CLASS IMBALANCE DETECTION\")\n",
        "#         print(\"-\" * 80)\n",
        "#         \n",
        "#         class_counts = df[target_col].value_counts()\n",
        "#         class_proportions = class_counts / len(df)\n",
        "#         \n",
        "#         print(\"\\nClass distribution:\")\n",
        "#         display(class_proportions.to_frame('Proportion'))\n",
        "#         \n",
        "#         # Check for imbalance (threshold: any class < 10%)\n",
        "#         min_proportion = class_proportions.min()\n",
        "#         if min_proportion < 0.1:\n",
        "#             findings['data_quality_issues'].append(\n",
        "#                 f\"Class imbalance detected: Minority class represents {min_proportion*100:.2f}% of data\"\n",
        "#             )\n",
        "#             findings['next_steps'].append(\"Consider using class balancing techniques (SMOTE, undersampling, etc.)\")\n",
        "\n",
        "# 5.3 Time-based Pattern Analysis\n",
        "datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()\n",
        "if len(datetime_cols) > 0:\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "    print(\"5.3 TIME-BASED PATTERN ANALYSIS\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    for col in datetime_cols[:2]:  # Limit to first 2 datetime columns\n",
        "        print(f\"\\nAnalyzing {col}...\")\n",
        "        \n",
        "        # Extract time components\n",
        "        df[f'{col}_year'] = pd.to_datetime(df[col]).dt.year\n",
        "        df[f'{col}_month'] = pd.to_datetime(df[col]).dt.month\n",
        "        df[f'{col}_day'] = pd.to_datetime(df[col]).dt.day\n",
        "        df[f'{col}_dayofweek'] = pd.to_datetime(df[col]).dt.dayofweek\n",
        "        \n",
        "        # Time series plot if we have a numerical target or feature\n",
        "        if len(numeric_cols) > 0:\n",
        "            # Plot first numerical column over time\n",
        "            plt.figure(figsize=(14, 6))\n",
        "            time_series = df.groupby(pd.to_datetime(df[col]).dt.date)[numeric_cols[0]].mean()\n",
        "            time_series.plot()\n",
        "            plt.title(f'{numeric_cols[0]} Over Time ({col})', fontsize=12, fontweight='bold')\n",
        "            plt.xlabel('Date', fontsize=10)\n",
        "            plt.ylabel(numeric_cols[0], fontsize=10)\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            \n",
        "            findings['feature_ideas'].append(f\"Extract time features from {col}: year, month, day, dayofweek, hour, etc.\")\n",
        "else:\n",
        "    print(\"\\nâœ“ No datetime columns found for time-based analysis\")\n",
        "\n",
        "# 5.4 Cluster and Grouping Identification\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"5.4 CLUSTER IDENTIFICATION PREPARATION\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\\nReview pairplot and correlation matrix for potential clusters or groups.\")\n",
        "print(\"Consider using dimensionality reduction (PCA, t-SNE) if needed.\")\n",
        "\n",
        "if len(numeric_cols) > 2:\n",
        "    findings['questions_for_team'].append(\"Should we apply dimensionality reduction techniques (PCA, t-SNE) for visualization?\")\n",
        "\n",
        "# 5.5 Preparation for Modeling Insights\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"5.5 MODELING PREPARATION INSIGHTS\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "modeling_insights = []\n",
        "\n",
        "# Feature count\n",
        "modeling_insights.append(f\"Total features available: {df.shape[1]}\")\n",
        "modeling_insights.append(f\"Numerical features: {len(numeric_cols)}\")\n",
        "modeling_insights.append(f\"Categorical features: {len(categorical_cols)}\")\n",
        "\n",
        "# Data size\n",
        "if df.shape[0] < 1000:\n",
        "    modeling_insights.append(\"Small dataset: Consider simpler models or data augmentation\")\n",
        "elif df.shape[0] > 100000:\n",
        "    modeling_insights.append(\"Large dataset: Can support complex models, consider sampling for faster iteration\")\n",
        "\n",
        "# Missing data impact\n",
        "if len(findings['data_quality_issues']) > 0:\n",
        "    modeling_insights.append(f\"Data quality issues to address: {len(findings['data_quality_issues'])}\")\n",
        "\n",
        "for insight in modeling_insights:\n",
        "    print(f\"â€¢ {insight}\")\n",
        "    findings['next_steps'].append(insight)\n",
        "\n",
        "print(\"\\nâœ“ Section 5: Multivariate Analysis & Patterns completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Feature Engineering Ideas\n",
        "\n",
        "**Team Activity:** Brainstorm together\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"FEATURE ENGINEERING IDEAS\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nUse this section to brainstorm and document feature engineering ideas.\")\n",
        "print(\"Add your ideas to the findings dictionary as you discuss.\")\n",
        "\n",
        "# 6.1 Numerical Feature Transformations\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"6.1 NUMERICAL FEATURE TRANSFORMATIONS\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "numerical_transforms = [\n",
        "    \"Polynomial features (xÂ², xÂ³) for non-linear relationships\",\n",
        "    \"Binning/bucketing for continuous variables\",\n",
        "    \"Log/Box-Cox transformations for skewed distributions\",\n",
        "    \"Standardization/Normalization (StandardScaler, MinMaxScaler)\",\n",
        "    \"Robust scaling for outlier-resistant normalization\",\n",
        "    \"Power transforms (square root, cube root)\"\n",
        "]\n",
        "\n",
        "print(\"\\nPotential transformations:\")\n",
        "for transform in numerical_transforms:\n",
        "    print(f\"  â€¢ {transform}\")\n",
        "\n",
        "# Document specific ideas based on your data\n",
        "for col in numeric_cols[:5]:  # Review first 5 numerical columns\n",
        "    skew_val = df[col].skew()\n",
        "    if abs(skew_val) > 1:\n",
        "        findings['feature_ideas'].append(f\"Apply log/Box-Cox transform to {col} (skewness={skew_val:.2f})\")\n",
        "\n",
        "# 6.2 Categorical Encoding Strategies\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"6.2 CATEGORICAL ENCODING STRATEGIES\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "categorical_encodings = [\n",
        "    \"One-hot encoding for low cardinality (< 10 unique values)\",\n",
        "    \"Label encoding for ordinal categories\",\n",
        "    \"Target encoding for high cardinality categories\",\n",
        "    \"Frequency encoding (replace with value counts)\",\n",
        "    \"Binary encoding for very high cardinality\",\n",
        "    \"Embedding for deep learning models\"\n",
        "]\n",
        "\n",
        "print(\"\\nPotential encoding strategies:\")\n",
        "for encoding in categorical_encodings:\n",
        "    print(f\"  â€¢ {encoding}\")\n",
        "\n",
        "# Document specific ideas based on your data\n",
        "for col in categorical_cols:\n",
        "    unique_count = df[col].nunique()\n",
        "    if unique_count < 10:\n",
        "        findings['feature_ideas'].append(f\"One-hot encode {col} ({unique_count} categories)\")\n",
        "    elif unique_count > 50:\n",
        "        findings['feature_ideas'].append(f\"Consider target/frequency encoding for {col} (high cardinality: {unique_count})\")\n",
        "\n",
        "# 6.3 Datetime Feature Extraction\n",
        "if len(datetime_cols) > 0:\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "    print(\"6.3 DATETIME FEATURE EXTRACTION\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    datetime_features = [\n",
        "        \"Extract: year, month, day, hour, minute, second\",\n",
        "        \"Extract: day of week, day of year, week of year\",\n",
        "        \"Extract: is_weekend, is_month_start, is_month_end\",\n",
        "        \"Extract: quarter, semester\",\n",
        "        \"Time since reference date\",\n",
        "        \"Cyclical encoding (sin/cos) for periodic features\"\n",
        "    ]\n",
        "    \n",
        "    print(\"\\nPotential datetime features:\")\n",
        "    for feature in datetime_features:\n",
        "        print(f\"  â€¢ {feature}\")\n",
        "    \n",
        "    for col in datetime_cols:\n",
        "        findings['feature_ideas'].append(f\"Extract time components from {col}: year, month, day, hour, dayofweek, etc.\")\n",
        "\n",
        "# 6.4 Domain-Specific Features\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"6.4 DOMAIN-SPECIFIC FEATURES\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\\nDiscuss domain knowledge and create relevant features.\")\n",
        "print(\"Examples:\")\n",
        "print(\"  â€¢ Ratio features (e.g., price per unit, density)\")\n",
        "print(\"  â€¢ Interaction features (e.g., product of two features)\")\n",
        "print(\"  â€¢ Aggregate features (e.g., mean, max, min by group)\")\n",
        "print(\"  â€¢ Distance/Similarity features\")\n",
        "print(\"  â€¢ Text features (if applicable): word count, sentiment, etc.\")\n",
        "\n",
        "# 6.5 Aggregate and Rolling Window Features\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"6.5 AGGREGATE & ROLLING WINDOW FEATURES\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "aggregate_features = [\n",
        "    \"Group-based aggregations (mean, median, std, min, max, count)\",\n",
        "    \"Rolling window statistics (moving average, rolling std)\",\n",
        "    \"Lag features (previous values)\",\n",
        "    \"Cumulative statistics\",\n",
        "    \"Rank-based features\"\n",
        "]\n",
        "\n",
        "print(\"\\nPotential aggregate features:\")\n",
        "for feature in aggregate_features:\n",
        "    print(f\"  â€¢ {feature}\")\n",
        "\n",
        "print(\"\\nâœ“ Section 6: Feature Engineering Ideas completed\")\n",
        "print(\"\\nðŸ’¡ TIP: Document your specific feature engineering ideas in the findings dictionary above.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: Summary & Action Items\n",
        "\n",
        "**Team Activity:** Review together and plan next steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"SUMMARY & ACTION ITEMS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 7.1 Data Quality Issues Summary\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"7.1 DATA QUALITY ISSUES\")\n",
        "print(\"-\" * 80)\n",
        "if len(findings['data_quality_issues']) > 0:\n",
        "    for i, issue in enumerate(findings['data_quality_issues'], 1):\n",
        "        print(f\"  {i}. {issue}\")\n",
        "else:\n",
        "    print(\"  âœ“ No major data quality issues identified\")\n",
        "\n",
        "# 7.2 Key Insights Summary\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"7.2 KEY INSIGHTS\")\n",
        "print(\"-\" * 80)\n",
        "if len(findings['key_insights']) > 0:\n",
        "    for i, insight in enumerate(findings['key_insights'], 1):\n",
        "        print(f\"  {i}. {insight}\")\n",
        "else:\n",
        "    print(\"  (No insights documented yet)\")\n",
        "\n",
        "# 7.3 Feature Engineering Ideas Summary\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"7.3 FEATURE ENGINEERING IDEAS\")\n",
        "print(\"-\" * 80)\n",
        "if len(findings['feature_ideas']) > 0:\n",
        "    for i, idea in enumerate(findings['feature_ideas'], 1):\n",
        "        print(f\"  {i}. {idea}\")\n",
        "else:\n",
        "    print(\"  (No feature ideas documented yet)\")\n",
        "\n",
        "# 7.4 Questions for Team Discussion\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"7.4 QUESTIONS FOR TEAM DISCUSSION\")\n",
        "print(\"-\" * 80)\n",
        "if len(findings['questions_for_team']) > 0:\n",
        "    for i, question in enumerate(findings['questions_for_team'], 1):\n",
        "        print(f\"  {i}. {question}\")\n",
        "else:\n",
        "    print(\"  (No questions documented yet)\")\n",
        "\n",
        "# 7.5 Next Steps\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"7.5 NEXT STEPS\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "next_steps_default = [\n",
        "    \"Address data quality issues (missing values, duplicates, outliers)\",\n",
        "    \"Implement feature engineering based on insights\",\n",
        "    \"Split data into train/validation/test sets\",\n",
        "    \"Select baseline models to try\",\n",
        "    \"Set up cross-validation strategy\",\n",
        "    \"Define evaluation metrics\",\n",
        "    \"Create modeling pipeline\"\n",
        "]\n",
        "\n",
        "all_next_steps = findings['next_steps'] + next_steps_default\n",
        "for i, step in enumerate(all_next_steps, 1):\n",
        "    print(f\"  {i}. {step}\")\n",
        "\n",
        "# 7.6 Export Findings to Text File\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"7.6 EXPORTING FINDINGS\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "export_content = f\"\"\"\n",
        "EDA FINDINGS REPORT\n",
        "===================\n",
        "Generated: {timestamp}\n",
        "Dataset: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\n",
        "\n",
        "DATA QUALITY ISSUES\n",
        "-------------------\n",
        "\"\"\"\n",
        "\n",
        "if len(findings['data_quality_issues']) > 0:\n",
        "    for i, issue in enumerate(findings['data_quality_issues'], 1):\n",
        "        export_content += f\"{i}. {issue}\\n\"\n",
        "else:\n",
        "    export_content += \"No major data quality issues identified.\\n\"\n",
        "\n",
        "export_content += f\"\\nKEY INSIGHTS\\n\"\n",
        "export_content += f\"{'=' * 20}\\n\"\n",
        "if len(findings['key_insights']) > 0:\n",
        "    for i, insight in enumerate(findings['key_insights'], 1):\n",
        "        export_content += f\"{i}. {insight}\\n\"\n",
        "else:\n",
        "    export_content += \"No insights documented.\\n\"\n",
        "\n",
        "export_content += f\"\\nFEATURE ENGINEERING IDEAS\\n\"\n",
        "export_content += f\"{'=' * 20}\\n\"\n",
        "if len(findings['feature_ideas']) > 0:\n",
        "    for i, idea in enumerate(findings['feature_ideas'], 1):\n",
        "        export_content += f\"{i}. {idea}\\n\"\n",
        "else:\n",
        "    export_content += \"No feature ideas documented.\\n\"\n",
        "\n",
        "export_content += f\"\\nQUESTIONS FOR TEAM DISCUSSION\\n\"\n",
        "export_content += f\"{'=' * 20}\\n\"\n",
        "if len(findings['questions_for_team']) > 0:\n",
        "    for i, question in enumerate(findings['questions_for_team'], 1):\n",
        "        export_content += f\"{i}. {question}\\n\"\n",
        "else:\n",
        "    export_content += \"No questions documented.\\n\"\n",
        "\n",
        "export_content += f\"\\nNEXT STEPS\\n\"\n",
        "export_content += f\"{'=' * 20}\\n\"\n",
        "for i, step in enumerate(all_next_steps, 1):\n",
        "    export_content += f\"{i}. {step}\\n\"\n",
        "\n",
        "# Write to file\n",
        "output_file = 'eda_findings.txt'\n",
        "with open(output_file, 'w') as f:\n",
        "    f.write(export_content)\n",
        "\n",
        "print(f\"\\nâœ“ Findings exported to: {output_file}\")\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"EDA COMPLETE!\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nNext: Review findings with team and proceed to feature engineering and modeling.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
